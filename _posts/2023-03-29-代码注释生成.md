---
layout: post
title:  "代码注释生成"
# date:   2023-03-29 08:04:00 +0800
tags: 业务安全
excerpt_separator: <!--more-->
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            
            }
        });
    </script>
</head>





在一篇关于代码注释生成的[综述](#1) 中，将注释生成大体上分为Information Retrieval Based、NN based 以及其他方法。本文主要关注基于NN的注释生成方法。代码注释生成是一个经典NL-PL问题，NL=natural language，PL=programming language。 <!--more--> 一个从sql到描述的例子：
![CodeNN SQL注释样例](/_posts/codenn.png "通过CodeNN生成SQL注释的例子")

# CodeNN

[CodeNN](#2) 基于LSTM+attention机制实现注释生成，注意力机制作用于LSTM状态向量与代码token embedding之间。模型接结构如下图。
其中 $c_i$ 表示代码tokend，$F$ 为其对应的Embedding。$n_i$ 表示注释token，$E$ 为其对应的Embedding。$h_i$表示LSTM的hidden state，$A$ 表示attention模块，$h_i$作为Q，代码的embedding作为K和V，得到向量$t_i$，与$h_i$相加用于预测注释token。
![CodeNN 结构](/_posts/codennstructure.png "CodeNN的主要结构")

一些实现细节：
- 的训练序列的开头结尾加入了 \<START> \<END> token，频率小于3的token统一为\<UKN>。
- SQL数据集代码token数为31,667，注释token数为7,470，数据对为32,337个；C#数据集代码token数为747，注释token数为2,506，数据对为66,015个。

# DeepCom
[DeepCom](#3) 相比 [CodeNN](#2) 有两点变化：
- 利用LSTM+attention构建Seq2Seq结构
- 引入基于AST(Abstract Syntax Tree)的结构特征

## 基于LSTM+attention的Seq2Seq模型结构
![DeepCom 结构](/_posts/deepcomstructure.png )
整体结构参照经典Encoder Decoder结构，焦点放在context vector和decoder输出部分$P(Y|X)$。Context vector的计算依赖attention，decoder的hidden state $h_{t-1}$作为Q，encoder的hidden state $s$ 作为K和V。输出部分如下估计$p(y_i|y_1,...,y_{i-1},x) = g(h_{t-1}, y_{t-1}, c_i)$，其中$c_i$为context vector。

## AST 和 SBT遍历
代码都可以抽象为[AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree)，AST的前序、中序遍历等常规遍历可能存在歧义：同一个遍历结果可能对应多种函数实现，因此提出SBT遍历。下面是一个例子。
![AST](/_posts/ast.png )

## 一些实现细节
- 使用的数据集是9714个github java项目，69708个method-doc pair
- Attention-based Seq2Seq 结构的引入带来了大部分增益，AST带来小幅增益
![seq2seq 增益](/_posts/seq2seqimprove.png )

# Hybrid2Seq_Attn_Drl 
该方法的主要改变是直接使用BLEU作为奖赏函数进行强化学习训练，使用Actor-Critc结构[Hybrid2Seq_attn_drl](#4)。 
![Hybrid2Seq](/_posts/hybrid2seq.png)
# [Code2Seq](#5)
![Code2seq](/_posts/code2seq.png)
## AST path encoder
从网络结构图的左侧可以看出，它针对每个path计算表示，再送入全连接层。
Path的选择方法：给定一个AST树，有众多叶子节点，这些叶子节点两两之间可以形成路径，在这些路径中随机采样若干个作为模型的输入。这样做的背后的原理是：实现同一个功能的两个不同代码，虽然整体AST会有差别，但在叶子结点之间的路径会保持一致性，这种表示能增加泛化性。
假设一个path可表示为$x=v_1,v_2,...v_l$，其中$v_1$，$v_l$为两个端点，整个path的表示由三部分组成：$path\\_encoder(x)$ 是一个Bi-LSTM， $node\\_encoder(v_1)$和$node\\_encoder(v_l)$分别是两个端点的表示。
## Token Representation 
Token Representation是一个Trick，也是为了提高泛化能力，例子是$ArrayList$ 会被拆分为$Array$和$List$。每个端点的表示会先把它拆分为token，在把每个token的embedding加起来。在后续很多工作中也使用了这个技巧。

# [Transformer Based]($6)
在原始Transformer基础上增加两个模块就超过当时主流方法：“We want to emphasize that our proposed approach is simple but effective as it outperformsthe fancy and sophisticated state-of-the-art sourcecode summarization techniques by a significant margin.”

1. Copy Attention 
模块的目的是允许模型从原始输入中拷贝一些关键词到输出中去。使用的方法来自\[[7](#7)]。
2. Pairwise relationship encoding
相对位置embedding通过在$V$和$K$两部分计算中引入可训练的$\alpha_{ij}^V$ 和 $\alpha_{ij}^K$：

<center>$ o_i = \sum_{j=1}^{n} \alpha_{ij}(x_jW^V+a_{ij}^V ) $</center>

<center>$ e_{ij} = \frac{x_i W^Q (x_j W^K+ \alpha_{ij}^K)}{\sqrt{(d_k)}}  $</center>

此外有一个观察：基于Transformer的模型对AST特征不敏感，加了没啥用。
# [Rencos](#9)
Rencos全称 Retrieval-based Neural Source Code Summarizer，在NMT方法基础上引入了Retrieval思路。
![Rencos](/_posts/rencos.png)
检索的步骤通过语法相似和语意相似两个通道完成，再加上测试样本，最终进入encoder-decoder的包含三个序列。Encoder使用Bi-LSTM，三路结果在decoder中分别输出概率，通过权重组合得到最终概率。
语法相似直接借助Lucene引擎，语意相似借助LSTM的隐藏状态做max_pooling后算cosine距离。

# [CodeBert](#8)
整个模型结构完全复用RoBERTa-base，具体操作包括：
1. Code与Doc的序列拼接表示为：$[CLS],w_1, ..., w_n,[SEP],c_1,...,c_m,[EOS]$
2. 用两个任务训练：MLM 和 RTD（Replaced Token Detection），RTD工作原理是：先用一个简单的token生成器（文中用n-gram生成）对一个位置生成候选token，替代掉原来token，模型的任务是对每个位置预测当前token是原始的还是生成的。
3. 用了2,137,293双模态数据，6,452,446代码单模态数据，包含go, java, js, php, python, ruby 代码。

作者在其他语言（C#）代码上也做了测试，效果只比 Code2Seq差一点，但没跟Transformer Based方法做对比。差的原因推测是没有引入AST信息。

# [PLBART](#10)
从名字上可以看出，它的结构采用BART，直观理解就是BERT encoder结果加上GPT decoder结构。训练任务是denoise：输入一个污染的序列，预测输出原始序列。PLBART中污染的方法有：token mask，删除，infilling。infilling指随机选一段token，用一个mask替代掉。这个预训练模型用到的数据量更是巨大：4.7亿个java文本，2.1亿个python文本，0.47亿个自然文本。
序列输入输入的形式如下图。
![PLBART](/_posts/plbart.png)


## 附录
<div id="1">[1] Song, X., Sun, H., Wang, X., &#38; Yan, J. (2019). A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques. <i>IEEE Access</i>, <i>7</i>, 111411–111428. https://doi.org/10.1109/ACCESS.2019.2931579 </div>

<div id="2">[2] Iyer, S., Konstas, I., Cheung, A., &#38; Zettlemoyer, L. (2016). Summarizing source code using a neural attention model. <i>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, 2073–2083.</div>

<div id="3">[3] Hu, X., Li, G., Xia, X., Lo, D., &#38; Jin, Z. (2018). Deep code comment generation. <i>Proceedings of the 26th Conference on Program Comprehension</i>, 200–210.</div>

<div id="4">[4] Wan, Y., Zhao, Z., Yang, M., Xu, G., Ying, H., Wu, J., &#38; Yu, P. S. (2018). Improving automatic source code summarization via deep reinforcement learning. <i>Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering</i>, 397–407.</div>

<div id="5">[5] Alon, U., Brody, S., Levy, O., &#38; Yahav, E. (2018). code2seq: Generating sequences from structured representations of code. <i>ArXiv Preprint ArXiv:1808.01400</i>.</div>

<div id="6">[6] Ahmad, W. U., Chakraborty, S., Ray, B., &#38; Chang, K.-W. (2020). A transformer-based approach for source code summarization. <i>ArXiv Preprint ArXiv:2005.00653</i>.</div>

<div id="7">[7] Nishida, K., Saito, I., Nishida, K., Shinoda, K., Otsuka, A., Asano, H., &#38; Tomita, J. (2019). <i>Multi-style Generative Reading Comprehension</i>.</div>

<div id="8">[8] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., &#38; Zhou, M. (2020). <i>CodeBERT: A Pre-Trained Model for Programming and Natural Languages</i>.</div>

<div id="9">[9] Zhang, J., Wang, X., Zhang, H., Sun, H., &#38; Liu, X. (2020). Retrieval-based neural source code summarization. <i>Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering</i>, 1385–1397.</div>

<div id="10">[10] Ahmad, W. U., Chakraborty, S., Ray, B., &#38; Chang, K.-W. (2021). Unified pre-training for program understanding and generation. <i>ArXiv Preprint ArXiv:2103.06333</i>.</div>