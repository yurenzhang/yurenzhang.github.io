---
layout: post
title:  "大模型基础Part1 - Position Encoding"
tags: 算法
excerpt_separator: <!--more-->
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            jax: ["input/TeX","output/SVG"],
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            
            }
        });
    </script>
</head>
<!--more-->

# Part1 Position Embedding 
在原始的Attention机制中，每个输入都会转化为$\mathbf{q}$和$\mathbf{k}$进行俩俩的点乘，但这个交互的过程中，位置信息是无法被体现出来的。简单来说，如果不考虑位置信息， "I am an AI" 与 "am I an AI" 在经过self-attention之后，每个token对应的输出会是相同的。这显然不符合我们对Transformer的预期。

因此要加入Position Embedding，它的作用是表征当前token所在的位置。对PE有什么要求呢？
- 它要能施加在输入向量上，比如加、乘、线性变换、拼接 都可以。
- 区分性：不同位置的Embedding应该不同。
- 连续性：相近位置的Embedding之间应该相似，距离远的Embedding之间应该有差别
- 有限性：不能太大，影响模型训练
- “平移不变性”：这个概念可能用的不准确，想表达的是，如果序列中的两个to\mathbf{k}en，他们的相对位置相同，则加入PE之后，他们在attention过程中的表现应该相同，特别是$q$和$k$点乘的结果应该相同。这种“平移不变形”能使Transformer在后续学习过程中，不会因为引入PE而对绝对位置上出现平移的两个Token之间的交叉产生不同的输出。“I `am` an `AI`” 和  “Hi, I `am` an `AI`”，这两句话中`am`和`AI`绝对位置发生了偏移，所以PE会不同，但他们的$\mathbf{q} \cdot \mathbf{k}$应该相同。

有几个自然而然的问题：
1. 为什么直接拼接一个当前位置的序号？
 直接使用序号，如果不做归一化则会产生超大的数字，在模型训练中会产生问题；如果归一化，则失去了“平移不变性”。
2. 为什么不用Hamming编码？
效率不够高，没有充分利用连续值的特性。
3. 为什么不用拼接的方式，而大多采用相加的方式把PE融进去？
没有明确的原因，但会造成向量长度翻倍，对效率没那么友好，特别是对大模型。

$$
  \begin{array}{lr}
    PE_i^k && i位置的k维位置编码 \\
    L && 序列长度 \\
    d && 输入embedding的维度 \\
    \mathbf{X} \in \mathbb{R}^{L*d} && 输入样本序列的特征
  \end{array}
$$

## 绝对位置编码 Sinusoidal Positional Encoding
第$i$位置的第$k$维 $PE_i^k$表示为 

$$PE_i^k = \begin{cases}
    \begin{array}{lr}
    sin(T^{2k'} * i) && k = 2k' \\
    cos(T^{2k'} * i) && k = 2k' + 1
    \end{array} 
    \end{cases}
$$

$$T = 1/10000^{1/d}$$ 

两个边界值，$k=0$第一维度, $PE = sin(i)$，$k=d$最后一维，$PE = sin(i/10000)$。

观察图中第1维（第一列）的表现，看到它周期较短，随纬度快速变化。第2维和第1维的周期相同，相位偏移。后面的维度周期变得越来越长。有点傅立叶变换的意思。 可以理解为：首先生成若干个波函数，频率不同，随后在每个位置上查波函数的取值，拼接为向量作为对该位置的表征。

如此计算的位置编码通过相加的方式叠加到输入embedding上，假设第$i$个位置的词的embedding是$\mathbf{x}_i^{emb}$，则叠加位置编码后的向量

$$\mathbf{x}_i = \mathbf{x}_i^{emb} + PE_i$$

在后续模型计算中提到的输入就是这个$\mathbf{x}_i$了。

![图](/_posts/positional_encoding.png)
<p style="color:gray;text-align: center"> 图1 绝对位置编码。每一行是一个位置的编码，每一列为一个维度。可以观察到每列的取值存在周期性。</p>

注意绝对位置编码的施加流程：

$$
\mathbf{x}_i = \mathbf{x}_i^{emb} + PE_i \\
\mathbf{q}_i = W^\mathbf{q}\mathbf{x}_i = W^\mathbf{q}(\mathbf{x}_i^{emb} + PE_i) \\
\mathbf{k}_j = W^\mathbf{k}\mathbf{x}_j = W^\mathbf{k}(\mathbf{x}_j^{emb} + PE_j) \\ 
\mathbf{v}_j = W^\mathbf{v}\mathbf{x}_j = W^\mathbf{v}(\mathbf{x}_j^{emb} + PE_j) \\
\mathbf{q}_i^T\mathbf{k}_j = (\mathbf{x}_i^{emb} + PE_i)^T W^{\mathbf{q}T} W^{\mathbf{k}} (\mathbf{x}_j^{emb} + PE_j)
$$

检查是否满足上面提到的所有标准：
1. 可施加在输入上：OK，维度与输入一致，相加即可
2. 区分性：OK，如图
3. 连续性：OK，相邻位置的距离是较小的，如果计算 位置间隔和向量距离之间的对应关系，可以看到一个比较平滑的曲线
4. 有限性：OK，都在0-1之间
5. 平移不变性：不具备。相同距离的两个位置的embedding互相之间都相差一个旋转矩阵线性变换，但由于是加到输入embedding上的，后面还有个$\mathbf{q}$和$K$矩阵相乘，所以并没有实现完全的平移不变性。

绝对位置编码满足大部分诉求，但在“平移不变性”上做的不是那么彻底。这也促使RoPE诞生。

## Rotary Position Embedding (RoPE)
如上所述，绝对位置编码对“平移不变性”的实现不够精准优雅，RoPE解决了这个问题。
RoPE的施加流程是：

$$
\mathbf{x}_i = \mathbf{x}_i^{emb} \\
\mathbf{q}_i =  R(i) W^\mathbf{q} \mathbf{x}_i^{emb} \\
\mathbf{k}_j =  R(j) W^\mathbf{k} \mathbf{x}_j^{emb} \\
\mathbf{v}_j =  R(j) W^\mathbf{v} \mathbf{x}_j^{emb} \\
\mathbf{q}_i\mathbf{k}_j^T =  \mathbf{x}_i^{embT} W^{\mathbf{q}T} R(i)^T R(j) W^{\mathbf{k}} \mathbf{x}_j^{emb}
$$

在2维空间中的旋转矩阵

$$
R(\theta) = \begin{bmatrix}
   cos(\theta) & -sin(\theta)  \\
   sin(\theta) & cos(\theta)  \\
\end{bmatrix}
$$

作用是将向量绕原点逆时针旋转$\theta$。 2维旋转矩阵的优良特性：
- $R(\theta)^T = R(\theta)^{-1} = R(-\theta)$
- $R(a) * R(b) = R(a+b)$
- 满足乘法交换律

在RoPE中，如下构建变换矩阵 

$$
R(i) = \begin{bmatrix}
   cos(i\theta_1) & -sin(i\theta_1) & 0 &  0 & ... & 0 & 0  \\
   sin(i\theta_1) & cos(i\theta_1)  & 0 &  0 & ... & 0 & 0 \\
    0 &  0 &sin(i\theta_2) & cos(i\theta_2)  & ... & 0 & 0 \\
   0 &  0 & sin(i\theta_2) & cos(i\theta_2)  & ... & 0 & 0 \\
   \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
   0 &  0  & 0 & 0 & ... &sin(i\theta_{d/2}) & cos(i\theta_{d/2}) \\
   0 &  0  & 0 & 0 & ... & sin(i\theta_{d/2}) & cos(i\theta_{d/2}) \\
\end{bmatrix}
$$

其中$\theta$的选择与绝对位置编码相同，本质上保证周期从小到大即可。
基于旋转矩阵的性质，容易得到 $R(i)^T = R(i)^{-1} = R(-i)$，因此 

$$\mathbf{q}_i^T\mathbf{k}_j = \mathbf{x}_i^{embT} W^{\mathbf{q}T} R(i)^T R(j) W^{\mathbf{k}} \mathbf{x}_j^{emb}  \\
 = \mathbf{x}_i^{embT} W^{\mathbf{q}T} R(i)^{-1} R(j) W^{\mathbf{k}} \mathbf{x}_j^{emb}\\
 = \mathbf{x}_i^{embT} W^{\mathbf{q}T} R(-i) R(j) W^{\mathbf{k}} \mathbf{x}_j^{emb} \\ 
 = \mathbf{x}_i^{embT} W^{\mathbf{q}T} R(j-i) W^\mathbf{k}\mathbf{x}_j^{emb} $$

可以看到，如果两个token出现的位置差$i-j$相同，则他们交叉结果必相同，与绝对位置无关，这就完全实现了“平移不变性”。

可以看到RoPE与绝对位置编码之间的区别：
- 绝对位置编码相加，RoPE乘在$\mathbf{q},\mathbf{k},\mathbf{v}$上
- 绝对位置编码不保证平移不变，RoPE能保证

## ATTENTION WITH LINEAR BIASES (ALiBi)
ALiBi要解决的是：更快的速度，更小的内存；能够更好的支持外推，在短序列上训练，在长序列上也能泛化。
方法简单粗暴：

$$
\mathbf{x}_i = \mathbf{x}_i^{emb} \\
\mathbf{q}_i = W^\mathbf{q}\mathbf{x}_i^{emb} \\
\mathbf{k}_j = W^\mathbf{k}\mathbf{x}_j^{emb} \\
\mathbf{v}_j = W^\mathbf{v}\mathbf{x}_j^{emb} \\
\mathbf{q}_i^T\mathbf{k}_j - m * (j-i) 代替 \mathbf{q}_i^T\mathbf{k}_j
$$

即做点积之后，再强制增加一个与位置差负相关的偏移。这必然保证了平移不变性，而且速度极快。
其中$m \in \\{ \frac{1}{2}, \frac{1}{2^2}, ..., \frac{1}{2^8} \\} $，对于8个头，每个头取不同值。如果多于8个头，则对幂次插值。


<div id="1">[1]
Biderman, S., Black, S., Foster, C., Gao, L., Hallahan, E., He, H., Wang, B., & Wang, P. (2021). Rotary Embeddings: A Relative Revolution. Retrieved from https://blog.eleuther.ai/rotary-embeddings/ </div>
<div id="2">[2] https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ </div>
<div id="3">[3] Shazeer, N. (2020). <i>GLU Variants Improve Transformer. </i> </div>