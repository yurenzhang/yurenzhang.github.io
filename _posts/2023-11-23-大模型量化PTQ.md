---
layout: post
title:  "大模型量化-PTQ"
tags: 算法
excerpt_separator: <!--more-->
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            jax: ["input/TeX","output/SVG"],
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            
            }
        });
    </script>
</head>
<!--more-->
# 大模型量化技术方案

大模型量化技术是一种新兴的人工智能技术，旨在降低大语言模型的运行时复杂度，提高性能和效率，从而能够更好地处理大规模数据，并有效地提供准确、快速的智能服务。目前，大语言模型量化技术主要包括量化(quantization)和知识蒸馏(knowledge distillation)两种方法，它们可以将模型的权重转换为较低精度的格式，从而在不损失预测准确性的前提下，大大减少了计算量和内存使用量。

量化的方法主要有两个，分别是Post Training Quantization(PTQ)和Quantization Aware Training(QAT)。 QAT 需要在训练过程中使用，暂时不考虑。PTQ对落地更友好一些，所以聚焦在PTQ方法。

该文章主要列举了一些具有较大影响力且可能进行工业落地的方法，可以作为论文检索的出发点，同时也可以快速了解主要的研究门类和研究脉络。 对每个方法主要关注：
1、压缩比例 & 性能降低幅度
2、推理速度
3、实现复杂度

# 基础

- 评判压缩后性能好坏的主要方法 Perplexity:

$$
PP(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}}
$$

    可以简单理解为对每个word的平均预测概率倒数，越小越好，越小说明模型给出的结果越笃定。

- 大模型很多使用半精度浮点数FP16运行，与全精度性能类似。

- 模型量化按权重和输入不同的量化程度，可以近似用 W4A8 W4A16 W8A8 的符号表示，比如 W8A8 表示模型里主要的权重用INT8 表示，主要的输入也用INT8 表示。但同时也注意，在各类量化模型里都会穿插使用FP16 或 FP32类型做中间值。
主流的方法中，SmoothQuant 属于 W8A8（activation 部分也做了8bit 量化）, GPTQ、AWQ和 QuantEase 为W8A16（输入 activation 为FP16），LLAMA.cpp的 Weight 量化部分是可变的，有若干个配置，activation 部分看代码也涉及FP16 部分，但不确定比例有多少。

- [The case for 4-bit precision:k-bit Inference Scaling Laws](#1)一文给出的结论是：首选4bit，参数占用空间和效果之间在4bit处取得最优，即同空间下各种精度的模型里，4bit模型的效果最好。

# 综述汇总
- [https://github.com/horseee/Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM)：NUS博士生维护的文章列表。
- [A Survey on Model Compression for Large Language Models](#2)

# Baseline方法Round To Nearest(RTN)

RTN最直接量化的方法，就是把值映射到离它最近的量化网格上。4Bit是RTN的极限，3Bit模型直接崩溃。
[LLAMA.cpp](https://github.com/ggerganov/llama.cpp/pull/1684)可能是目前最好的RTN实现方案。
LLAMA.cpp里对不同的配置（group 大小，不同类型层的量化方法，量化位数等）下的结果做评估，找到了相对较好的平衡点。
LLAMA.cpp 不同量化配置下的效果对比如下：

Q4_0   :  3.50G, +0.2499 ppl @ 7B - small, very high quality loss - legacy, prefer using Q3_K_M

Q4_1   :  3.90G, +0.1846 ppl @ 7B - small, substantial quality loss - legacy, prefer using Q3_K_L

Q5_0   :  4.30G, +0.0796 ppl @ 7B - medium, balanced quality - legacy, prefer using Q4_K_M

Q5_1   :  4.70G, +0.0415 ppl @ 7B - medium, low quality loss - legacy, prefer using Q5_K_M

Q2_K   :  2.67G, +0.8698 ppl @ 7B - smallest, extreme quality loss - not recommended

Q3_K_S :  2.75G, +0.5505 ppl @ 7B - very small, very high quality loss

Q3_K_M :  3.06G, +0.2437 ppl @ 7B - very small, very high quality loss

Q3_K_L :  3.35G, +0.1803 ppl @ 7B - small, substantial quality loss

Q4_K   : alias for Q4_K_M

Q4_K_S :  3.56G, +0.1149 ppl @ 7B - small, significant quality loss

Q4_K_M :  3.80G, +0.0535 ppl @ 7B - medium, balanced quality - recommended

Q5_K   : alias for Q5_K_M

Q5_K_S :  4.33G, +0.0353 ppl @ 7B - large, low quality loss - recommended

Q5_K_M :  4.45G, +0.0142 ppl @ 7B - large, very low quality loss - recommended

Q6_K   :  5.15G, +0.0044 ppl @ 7B - very large, extremely low quality loss

Q8_0   :  6.70G, +0.0004 ppl @ 7B - very large, extremely low quality loss - not recommended

F16    : 13.00G              @ 7B - extremely large, virtually no quality loss - not recommended

F32    : 26.00G              @ 7B - absolutely huge, lossless - not recommended

其中每行表示一种量化的配置（group 大小，不同类型层的量化方法，量化位数等），比如 TYPE_Q4_K表示4bit 带偏置的量化，32 个weight 作为一个block 做量化，scale 和偏置使用 6bit 量化，最终压缩到4.5 bpw。 

此外，LLAMA.cpp也支持加载GPTQ了。

# GPTQ系列

### [GPTQ](#3)

```Highlight```

- bit数降低到3或4位，突破RTN没法压缩到 3bit 的限制

- GPT 175B 共326G，被塞到了80G的A100里，perplexity 基本不增加

- One-shot，不需多余操作

```基础知识```
[Cholesky Reformulation](https://en.wikipedia.org/wiki/Cholesky_decomposition)，将正定矩阵分解为下三角矩阵和它转置的乘积

```方法```

1. Optimal brain quantization(OBQ)： 找到对$A=WX$结果影响最小的量化维度，并将它量化。它的量化会造成$WX$结果轻微的变化，为了补偿这个变化，对其他没量化的维度做更新，可以通过公式计算出需要更新的量。问题是每次算一个权重，非常慢。

2. GPTQ：首先不再找影响最小的维度去更新了，而是按顺序全部更新；其次，按BLOCK进行更新，即一次更新若干个相邻的维度，期间不对整体的矩阵做更新，只有在一个BLOCK的量化都完成的时候，才去更新整体的矩阵，从而提高计算的效率；最后，用Cholesky Reformulation 解决了海森矩阵$H^{-1}$计算不稳定的问题

```实现``` [GPTQ for LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)

```点评``` 逻辑比较清楚，有明确的优化目标，有比较多开源的实现，是目前比较主流的方法。

### [SpQR](#4)

```Highlight```
- 在 GPTQ 基础上增加了对异常值的考虑
- 不仅考虑了activation的异常值的问题，也考虑了weight中异常值的问题。weight中的异常值分布没有 input那么规律
- 双层量化，scale 和 bias 又做了一层量化
- 做到了比 FP16 更快的推理速度（20-30%提升）



```方法```
1. 先尝试用GPTQ压缩到3Bit
2. 识别到了W矩阵中的几种异常模式： 行列异常值、rotary 循环、Stripes
3. 利用16bit和低bit量化后造成的差别评估一个权重是不是异常的，异常的权重单独处理
4. 对于非异常的的权重区域，使用双层的Group量化：一组特征复用一套量化参数，一组量化参数再一起做一次量化
5. 对于异常的权重区域，保留FP16 的精度

```点评``` 为什么有 dequantize 的过程反而速度快呢？解释是内存占用低了，而 inference 主要是内存受限，同内存可以塞入更多的内容，从总并发上看可以掩盖dequantize 造成的成本。

### [QuantEase](#5)

```Highlights```

- QuantEase可以将语言模型有效地压缩到3-4比特,其相对量化误差较其他方法GPTQ等降低了30%
- 可以 V100 单卡量化66B 模型，GPTQ 和 AWQ 会OOM


```知识点``` Cyclic Coordinate Descending 

```方案```

1. 挨个权重做量化，并始终以最小化误差为目标，直到所有的权重都被量化，如此往复

2. 每个权重优化的方法是：先不考虑量化问题，找到最优解再量化就可以了

3. 发现：同一列的weight的变化理论上互不影响（矩阵乘法），所以可以一列一列地做优化

4. 每三次迭代，取消一次量化操作

5. 借鉴 AWQ 的思路：在优化的过程中尝试找到sparse的需要保持原有精度的weights，方法是：在做CD迭代的过程中，穿插寻找需要保留的稀疏参数。

6. 13B模型 需要 33.8H，V100 

```点评``` 是GPTQ 和 AWQ的结合，效果看起来更好，思路也比较简洁，可能有比较大的应用空间；没有Code，比较可惜。

# 针对异常值特殊关照 

### [LLM.int8()](#6)

```Highlight```
- 比较早的有影响的关注异常值的量化工作：在activation中存在规律行的异常之，他们对最终效果有比较大的影响

```方法```

1. Vector-wise quant: $WX，$X$每个特征共享一组缩放参数，$W$每行共享一个组缩放参数。
2. X只有特定几个维度上容易出现极大值，对他们做特殊处理就好了，即对每特征里，抠出来几维度单独按 FP16 算，其他的按8bit缩放处理

```点评``` 速度比FP16慢一些，因为多了一些矩阵拆解、dequant的操作；8bit不是最优量化选择，实际应用可以不做考虑。

### [AWQ](#7)

```Highlight```
- 跟LLM.int8 非常的相似的思路，找到显著的激活值
- 不同点是没把异常值的部分单独拉出来，而是统一通过缩放

```方法```
1. 缩放$W$和$X$再做量化：部分Activeation比较显著，一般对$W$进行放大，对$X$进行缩小，可以使两边的值域空间都更小，从而增加量化过程中对显著通道的解析分辨率，从而提高整体perplexity
2. 过大的缩放也会导致non-salient通道的特征被过度的忽视，也会降低模型整体的性能

```点评``` 过程比较粗糙，直观感觉不太可靠，但效果挺好。

### [SmoothQuant](#8)

```Highlight``` W8A8 定制化实现速度快

```方法```

1. 同样关注到了activation大的情况
2. 在Tensor尺度上把weight的值域加大，把输入的值域压缩，通过gridsearch找到各个模型合适的尺度
3. 真正的W8A8， 基于 GEMM/GEMV 实现了int8*int8 。相比之下，GPTQ 需要反量化到fp16 计算，所以GPTQ会慢一些。

```点评``` 感觉跟AWQ异曲同工，但对比的工作太少了，也担心A8的操作导致不必要的损失。

# 文章中效果汇总

```4bit下的效果对比```

QuantEase > AWQ ~ GPTQ > RTN [[5]](#5)

SpQR > GPTQ > RTN  [[4]](#4)

LLAMA.cpp Q4_K_M > GPTQ for 7B 13B 30B [llama.cpp github issue](https://github.com/ggerganov/llama.cpp/issues/9) 和 [a comparison](https://oobabooga.github.io/blog/posts/perplexities/)

```推理速度```

速度比较玄学，一方面不同方法用各自的加速Trick，比如替换cuDNN的实现、使用VRAM加速、替换数据类型实现等，另一方面，同样算法在不同架构的GPU上推理速度也不一样，取决于GPU对不同类型数据的支持能力不同，比如A10在整数计算上就不大行。 

# 开源代码
[ExLlama](https://github.com/turboderp/exllama) GPTQ 实现

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) LLM.int8()官方实现

[Intel的压缩工具](https://github.com/intel/neural-compressor) 支持SmoothQuant

[LLAMA.cpp](https://github.com/ggerganov/llama.cpp)

# 其他的一些工作

- [ZeroQuant](#9)： 边量化边蒸馏，又干不过GPTQ，PASS了
- [RPTQ](#10)：把值域类似的通道聚类共享量化参数，做行列的重排实现加速，做到了3Bit；实现非常的复杂
- [Olive](#11) ：把异常值和非异常值合并在一起量化，以实现更高的压缩率。这个操作比较fancy，也没跟主流的量化方法做对比
- [FPTQ](#12)：实现了W4A8，空间和速度上理论上会更好，但效果距离W4A16始终有一定距离

# 总结
4bit 首选；LLAMA.cpp 首选；看好QuantEase。


<div id="1">[1] Dettmers, Tim, and Luke Zettlemoyer. "The case for 4-bit precision: k-bit inference scaling laws, 2022." URL https://arxiv. org/abs/2212.09720. </div>

<div id="2">[2] Zhu, Xunyu, et al. "A survey on model compression for large language models." arXiv preprint arXiv:2308.07633 (2023).</div>

<div id="3">[3] Frantar, Elias, et al. "Gptq: Accurate post-training quantization for generative pre-trained transformers." arXiv preprint arXiv:2210.17323 (2022).</div>
<div id="4">[4] Dettmers, Tim, et al. "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression." arXiv preprint arXiv:2306.03078 (2023).</div>
<div id="5">[5] Behdin, Kayhan, et al. "QuantEase: Optimization-based Quantization for Language Models--An Efficient and Intuitive Algorithm." arXiv preprint arXiv:2309.01885 (2023).</div>
<div id="6">[6] Dettmers, Tim, et al. "Llm. int8 (): 8-bit matrix multiplication for transformers at scale." arXiv preprint arXiv:2208.07339 (2022).</div>
<div id="7">[7] Lin, Ji, et al. "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration." arXiv preprint arXiv:2306.00978 (2023). </div>
<div id="8">[8] Xiao, Guangxuan, et al. "Smoothquant: Accurate and efficient post-training quantization for large language models." International Conference on Machine Learning. PMLR, 2023.</div>
<div id="9">[9] Yao, Zhewei, et al. "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers." Advances in Neural Information Processing Systems 35 (2022): 27168-27183.</div>
<div id="10">[10] Yuan, Zhihang, et al. "RPTQ: Reorder-based Post-training Quantization for Large Language Models." arXiv preprint arXiv:2304.01089 (2023).</div>
<div id="11">[11] Guo, Cong, et al. "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization." Proceedings of the 50th Annual International Symposium on Computer Architecture. 2023.</div>
<div id="12">[12] Li, Qingyuan, et al. "Fptq: Fine-grained post-training quantization for large language models." arXiv preprint arXiv:2308.15987 (2023).</div>