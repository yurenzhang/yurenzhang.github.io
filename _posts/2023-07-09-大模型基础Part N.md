---
layout: post
title:  "大模型基础Part2 - GLU"
tags: 算法
excerpt_separator: <!--more-->
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            jax: ["input/TeX","output/SVG"],
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            
            }
        });
    </script>
</head>
<!--more-->

# Part3 GLU
最原始的激活函数是Sigmoid，后来在CNN应用中ReLU（Rectified Linear Unit）和其变种逐渐流行起来。而在Transformer架构中，以GLU（Gated Linear Unit) 为代表的一些新的非线性单元逐渐流行起来。

## 基本原理
聚集在Transformer中的FFN层。其FFN一般为两层。

$$ 
FFN_{ReLU}(x,W_1,W_2, b_1, b_2) = max(0, xW_1 + b_1)W_2 + b_2
$$

应用中可以把bias全去掉，这样表示也清爽些，后面也都做相同处理。

$$ 
FFN_{ReLU}(x,W_1,W_2) = max(0, xW_1)W_2
$$

下面是GLU及一些变种的公式：

$$
\begin{array}{rl}
    GLU(x, W, V) &=& \sigma(xW) \otimes xV \\
    GEGLU(x, W, V) &=& GELU(xW) \otimes xV  \\
    SwiGLU(x, W, V, \beta) &=&  Swish_{\beta}(xW) \otimes xV 
  \end{array}
$$

对应的两层FFN公式是：

$$
\begin{array}{rl}
FFN_{GLU} &=& (\sigma(xW) \otimes xV) W_2 \\ 
FFN_{GEGLU} &=& (GELU(xW) \otimes xV) W_2 \\
FFN_{SwiGLU} &=& (Swish_{\beta}(xW) \otimes xV ) W_2 = (xW \otimes sigmoid(xW) \otimes xV) W_2
\end{array}
$$

解析：
- GELU(Gaussian Error Linear Unit)函数来自于 [[2]](#2)， $GELU(x) = xP(X \le x) = x\Phi(x)$，其中$\Phi$是高斯分布的CDF。设计背后的直觉是：输入越大（在高斯分布的右侧）则越要保留它的影响，输入越小（在高斯分布的左侧）减弱它的影响。从图像上看，GELU跟ReLU类似，但更平滑。应用中用近似的函数替代。
- $Swish_{\beta} = x \cdot sigmoid(\beta x)$，虽然$\beta$可以学习，但一般令$\beta=1$，$Swith(x) = x \cdot sigmoid(x)$。
- 注意 GELU 和 Swish 单独也可以用作激活函数，其形态都与ReLU和Sigmoid有相似之处。GPT用GELU。
- 实验里Swish 和 GEGLU 效果较好。
- 注意GLU系列方法引入了新的可训练参数。

## GLU与Sigmoid相比主要增加了什么特性？
GLU的主要特点在于Gated，即刻意通过门控函数调节通过的信息，去伪存精。但为什么work？没人知道。



<div id=1>[1] Google, N. S. (2020). <i>GLU Variants Improve Transformer</i>. https://arxiv.org/abs/2002.05202v1</div>
<div id=2>[2] Hendrycks, D., &#38; Gimpel, K. (2016). <i>Gaussian Error Linear Units (GELUs)</i>. https://arxiv.org/abs/1606.08415v5</div>